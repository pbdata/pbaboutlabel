{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Feature Enginnering ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Aboutlabeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Article', 'Line']] = df['ID'].str.split(pat='.', n=1, expand=True).values\n",
    "del df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Offsets'] = df['Offsets'].apply(lambda s: ast.literal_eval(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Multi label in Fasttext format\n",
    "df['Labels'] = df['Offsets'].apply(lambda l: ' '.join(list(set(['__label__' + i['label'] for i in l]))))\n",
    "df.loc[df['Label'] == 'None', 'Labels'] = '__label__None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data and toneize\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '',' '.join(nltk.word_tokenize(x.lower()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining in the Fasttext format\n",
    "df['FastText'] = df['Labels'] + ' ' + df['Text']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Files into Train and Validation\n",
    "df.head(13625)[['FastText']].to_csv('pb.train.txt', header=None, index=None, quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")\n",
    "df.tail(3028)[['FastText']].to_csv('pb.valid.txt', header=None, index=None, quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Hyperopt ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Classfier for Registring the model in MLFlow\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "y = np.array([1, 1, 2, 2])\n",
    "from sklearn.svm import SVC\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting MLflow\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import mlflow\n",
    "tracking_uri = 'http://mlflo-mlflo-kg1i011s8hid-60b8dc955cae2952.elb.us-east-1.amazonaws.com'\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "experiment_name = 'pitchbook-about'\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperopt objective function\n",
    "from sklearn.metrics import f1_score\n",
    "import fasttext\n",
    "\n",
    "valid = pd.read_csv(\"pb.valid.txt\", header=None, sep='\\t', names=['text'])\n",
    "valid['text'] = valid['text'].str.split(' ').apply(lambda l: ' '.join([i for i in l if '__label__' not in i]))\n",
    "\n",
    "def objective(params):\n",
    "    lr = params['lr']\n",
    "    epoch = int(params['epoch'])\n",
    "    wordNgrams = int(params['wordNgrams'])\n",
    "    threshold = params['threshold']\n",
    "    \n",
    "    model = fasttext.train_supervised(input=\"pb.train.txt\", loss='ova', lr=lr, epoch=epoch, wordNgrams=wordNgrams)\n",
    "   \n",
    "    predictions = []\n",
    "    for i in range(valid.shape[0]):\n",
    "        pred = model.predict(valid['text'][i], k=-1, threshold=threshold)\n",
    "        predictions.append((list(pred[0]).__len__() == 1) & ('__label__None' in list(pred[0])))\n",
    "\n",
    "    predictions = ['None' if i else 'About' for i in predictions]\n",
    "    \n",
    "    actuals = df.tail(3028).copy()\n",
    "    actuals['predictions'] = predictions\n",
    "    \n",
    "    y_true = actuals['Label'].copy()\n",
    "    y_pred = actuals['predictions'].copy()\n",
    "    \n",
    "    score = f1_score(y_true, y_pred, average=None)[0]\n",
    "    \n",
    "    return {'loss': -score, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Hyperopt search space\n",
    "space = { \n",
    "    'lr': hp.uniform('lr', 0.1, 1),\n",
    "    'epoch': hp.quniform('epoch', 5, 50, 1),\n",
    "    'wordNgrams': hp.quniform('wordNgrams', 1, 6, 1),\n",
    "    'threshold': hp.uniform('threshold', 0.01, 0.4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Hyperopt with MLflow tracking\n",
    "trials = Trials()\n",
    "\n",
    "with mlflow.start_run(run_name='nltk-multi-label'):\n",
    "    argmin = fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,\n",
    "        trials=trials)  \n",
    "    \n",
    "    lr = argmin['lr']\n",
    "    epoch = int(argmin['epoch'])\n",
    "    wordNgrams = int(argmin['wordNgrams'])\n",
    "    threshold = argmin['threshold']\n",
    "    \n",
    "    model = fasttext.train_supervised(input=\"pb.train.txt\", loss='ova', lr=lr, epoch=epoch, wordNgrams=wordNgrams)\n",
    "   \n",
    "    predictions = []\n",
    "    for i in range(valid.shape[0]):\n",
    "        pred = model.predict(valid['text'][i], k=-1, threshold=threshold)\n",
    "        predictions.append((list(pred[0]).__len__() == 1) & ('__label__None' in list(pred[0])))\n",
    "\n",
    "    predictions = ['None' if i else 'About' for i in predictions]\n",
    "    \n",
    "    actuals = df.tail(3028).copy()\n",
    "    actuals['predictions'] = predictions\n",
    "    \n",
    "    y_true = actuals['Label'].copy()\n",
    "    y_pred = actuals['predictions'].copy()\n",
    "    \n",
    "    score = f1_score(y_true, y_pred, average=None)[0]\n",
    "\n",
    "    mlflow.log_param(\"lr\", lr)\n",
    "    mlflow.log_param(\"epoch\", epoch)\n",
    "    mlflow.log_param(\"wordNgrams\", wordNgrams)\n",
    "    mlflow.log_param(\"threshold\", threshold)  \n",
    "    \n",
    "    mlflow.log_metric(\"f1_score\", score)\n",
    "    mlflow.sklearn.log_model(clf, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Final Model ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model using optimized hyperparameters\n",
    "import fasttext\n",
    "model = fasttext.train_supervised(input=\"pb.train.txt\", lr=\t0.7468864248632575, epoch=27, wordNgrams=4, loss='ova')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a single text inference on local model\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text = \"Early on, ORCO changed its name to Organic Dyestuffs Corporation and, in 2014, further changed the name to Organic Dyes and Pigments LLC, reflecting its well established heritage in both pigments and dyes.\"\n",
    "text =  re.sub(r'[^a-zA-Z0-9\\s]', '',' '.join(nltk.word_tokenize(text.lower())))\n",
    "\n",
    "pred = model.predict(text, k=-1, threshold=0.018211034677959557)\n",
    "pred_bool = (list(pred[0]).__len__() == 1) & ('__label__None' in list(pred[0]))\n",
    "\n",
    "if pred_bool:\n",
    "    print('None')\n",
    "else:\n",
    "    print('About')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to deploy to Sagemaker\n",
    "model.save_model(\"final_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Deployment #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Sagemaker defaults \n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"fasttext/pretrained\"\n",
    "region_name = 'us-east-1'\n",
    "container = sagemaker.amazon.amazon_estimator.image_uris.retrieve(\"blazingtext\", region_name, \"1\")\n",
    "model_location = 's3://sagemaker-us-east-1-943579580584/fasttext/pretrained/final_model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "pb_about = sagemaker.Model(\n",
    "    image_uri=container, model_data=model_location, role=role, sagemaker_session=sess\n",
    ")\n",
    "pb_about.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n",
    "\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=pb_about.endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a single text inference on deployed Sagemaker model using Sagemaker SDK\n",
    "\n",
    "text = \"Headquartered\"\n",
    "text =  re.sub(r'[^a-zA-Z0-9\\s]', '',' '.join(nltk.word_tokenize(text.lower())))\n",
    "\n",
    "sentences = [text]\n",
    "\n",
    "payload = {\"instances\": sentences,\n",
    "          \"configuration\": {\"k\": 5}}\n",
    "\n",
    "predictions = predictor.predict(payload)\n",
    "predictions = pd.DataFrame.from_dict(predictions[0])\n",
    "\n",
    "if ('__label__None' in predictions['label'].values) & (predictions[predictions['label'] == '__label__None']['prob'].values[0] > 0.999):\n",
    "    print('None')\n",
    "else:\n",
    "    print(\"About\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a single text inference on deployed Sagemaker model using Python SDK boto3\n",
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import json\n",
    "import csv\n",
    "\n",
    "runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "text = \"David founded Independent Financial Consultants (IFC Finance.com), a successful Financial Services Advisory firm which provides consultancy on Financial Planning and Wealth Management to business owners and professional firms, in Ireland and UK.\"\n",
    "text =  re.sub(r'[^a-zA-Z0-9\\s]', '',' '.join(nltk.word_tokenize(text.lower())))\n",
    "sentences = [text]\n",
    "payload = {\"instances\": sentences,\n",
    "          \"configuration\": {\"k\": 5}}\n",
    "payload = json.dumps(payload, indent = 4)\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=\"blazingtext-2021-08-09-17-57-44-099\",\n",
    "                                   ContentType='application/JSON',\n",
    "                                   Body=payload)\n",
    "\n",
    "predictions = json.loads(response['Body'].read().decode())\n",
    "predictions = pd.DataFrame.from_dict(predictions[0])\n",
    "\n",
    "if ('__label__None' in predictions['label'].values) & (predictions[predictions['label'] == '__label__None']['prob'].values[0] > 0.999):\n",
    "    predicted_label = {'label': 'None'}\n",
    "else:\n",
    "    predicted_label = {'label': 'About'}\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Lambda and API Gateway ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.data.path.append(\"/tmp\")\n",
    "nltk.download(\"punkt\", download_dir = \"/tmp\")\n",
    "\n",
    "# grab environment variables\n",
    "ENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n",
    "runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(\"Received event: \" + json.dumps(event, indent=2))\n",
    "    \n",
    "    data = json.loads(json.dumps(event))\n",
    "    text = data['data']\n",
    "    print(text)\n",
    "    \n",
    "    text =  re.sub(r'[^a-zA-Z0-9\\s]', '',' '.join(nltk.word_tokenize(text.lower())))\n",
    "    sentences = [text]\n",
    "    payload = {\"instances\": sentences,\n",
    "              \"configuration\": {\"k\": 5}}\n",
    "    payload = json.dumps(payload, indent = 4)\n",
    "\n",
    "    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                       ContentType='application/JSON',\n",
    "                                       Body=payload)\n",
    "\n",
    "    predictions = json.loads(response['Body'].read().decode())\n",
    "    predictions = pd.DataFrame.from_dict(predictions[0])\n",
    "\n",
    "    if ('__label__None' in predictions['label'].values) & (predictions[predictions['label'] == '__label__None']['prob'].values[0] > 0.999):\n",
    "        predicted_label = {'label': 'None'}\n",
    "    else:\n",
    "        predicted_label = {'label': 'About'}\n",
    "    print(predicted_label)\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### GitHub CI/CD #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import fasttext\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def returntestscore():\n",
    "    df = pd.read_csv('Aboutlabeled.csv')\n",
    "\n",
    "    df[['Article', 'Line']] = df['ID'].str.split(pat='.', n=1, expand=True).values\n",
    "    del df['ID']\n",
    "\n",
    "    df['Offsets'] = df['Offsets'].apply(lambda s: ast.literal_eval(s))\n",
    "\n",
    "    df['Labels'] = df['Offsets'].apply(lambda l: ' '.join(list(set(['__label__' + i['label'] for i in l]))))\n",
    "\n",
    "    df.loc[df['Label'] == 'None', 'Labels'] = '__label__None'\n",
    "\n",
    "    df['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '',' '.join(nltk.word_tokenize(x.lower()))))\n",
    "\n",
    "    df['FastText'] = df['Labels'] + ' ' + df['Text']\n",
    "\n",
    "    df.head(13625)[['FastText']].to_csv('pb.train.txt', header=None, index=None, quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")\n",
    "    df.tail(3028)[['FastText']].to_csv('pb.valid.txt', header=None, index=None, quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")\n",
    "\n",
    "    ################## Final Model ###################################\n",
    "    model = fasttext.train_supervised(input=\"pb.train.txt\", lr=\t0.7468864248632575, epoch=27, wordNgrams=4, loss='ova')\n",
    "\n",
    "    valid = pd.read_csv(\"pb.valid.txt\", header=None, sep='\\t', names=['text'])\n",
    "    valid['text'] = valid['text'].str.split(' ').apply(lambda l: ' '.join([i for i in l if '__label__' not in i]))\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(valid.shape[0]):\n",
    "        pred = model.predict(valid['text'][i], k=-1, threshold=0.018211034677959557)\n",
    "        predictions.append((list(pred[0]).__len__() == 1) & ('__label__None' in list(pred[0])))\n",
    "\n",
    "    predictions = ['None' if i else 'About' for i in predictions]\n",
    "\n",
    "    actuals = df.tail(3028).copy()\n",
    "    actuals['predictions'] = predictions\n",
    "\n",
    "    y_true = actuals['Label'].copy()\n",
    "    y_pred = actuals['predictions'].copy()\n",
    "\n",
    "    score = f1_score(y_true, y_pred, average=None)[0]\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Unit test file for app.py\"\"\"\n",
    "from app import returntestscore\n",
    "import unittest\n",
    "\n",
    "class TestApp(unittest.TestCase):\n",
    "    \"\"\"Unit tests defined for app.py\"\"\"\n",
    "\n",
    "    def test_f1_score(self):\n",
    "        \"\"\"Test f1 score\"\"\"\n",
    "        score = returntestscore()\n",
    "        self.assertGreater(score, 0.5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
